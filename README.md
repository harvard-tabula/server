## Notes for both Development and Production

### Reqirements

For both development and production this project uses [docker-compose](https://docs.docker.com/compose/). In dev the relevant file is `docker-compose.yml`. Prod uses `docker-compose-prod.yml`. Using Docker makes it easy to swap out different components. Right now this means that in dev we're using a container for the DB and another for the data volume, while in prod the DB is swapped out for RDS. Yay modular components.

Example environment files are included in the repository. See `.dev-example` and `.env-example`. 

You'll also need to include a certificate called `certificate.pem` and private key called `key.key` in the root of the project. The key pair is generated by CloudFlare. If you change the names above you'll need to update the Dockerfile as well as the NGINX config found in the `sites-enabled` directory.

### Philosophy 

This is purely an API server. It knows nothing about the structure of the [frontend](https://github.com/harvard-tabula/frontend) and has no control over rendering or routing. Because of this, some of the responses may seem a little odd. For example, when a user needs to be redirected (say for authentication purposes) the server will return something like this:

```
{
    "message": "The browser must be manually redirected",
    "redirect": "https://accounts.google.com/o/oauth2/..........",
    "state": 302
}
```
In development this means you will need to manually copy the redirect link into the browser and hit return. This kind of issues arises because of the slow-uptake of new standards such as the Fetch API. While the browser knows how to handle a standard `302`, `fetch` does not (at least for now).  


### Authentication

This project uses cookies for authentication. We *highly* recommend installing the Postman apps to test the API. The file `public.api` is the latest Postman export, which you can use to prepopulate all of the relevant endpoints, see examples, and read documentation. If you're using the Chrome app, you'll want to install the [interceptor extension](https://www.getpostman.com/docs/interceptor_cookies) to deal with cookies. Either way, keep in mind that you may need to hit `<base_url>/login` in the browser to get authenticated where in development `<base_url>` should be `http://tabula.life` if you're following our workflow. In production `<base_url>` is `https://api.tabula.life`. 

### Dependencies

There are multiple cloud services this product relies on:

* AWS EC2 and RDS
* Cloudflare for SSL (domain purchased through NameCheap) and DNS admin
* Google OAuth (which means you need to have the Developer Console set up) 

It also stitches together a bunch of technologies you'll probably want to familiarize yourself with, especially: 

* NGINX
* uWSGI 
* Flask 
* Postgres
* Docker
* Alembic

## Dev

### Before you begin
* Install the latest versions of Docker and docker-compose
* Specify environment variables in `.dev`. 
* To handle OAuth you'll need to update `/etc/hosts` on Mac (or the equivalent for your OS) to add a mapping from `0.0.0.0` (which is the host the server is forwarded to on your local machine) to a domain currenty registered on [Joe](https://www.github.com/josephwandile)'s Google Developer Console as an Authorized Redirect URL. Ideally use the mapping from  `0.0.0.0` to `tabula.life`. If you really want to use a different mapping ask Joe to add it to the developer console.
* Make sure you've got the TLS key pair in the root directory. For the moment development doesn't use `https`, but we'd rather not have separate Dockerfiles for dev and prod since the added time taken to copy the certificates onto the dev container even though they're not being used is negligible.
* Make sure the line `ssl on;` in `sites-enabled/tabula` is commented out or NGINX will try to enforce `https` even though the certificates will be considered invalid by the browser.

### Getting started
* Navigate to the project root and run `docker-compose build`. This will create all of the containers we care about. 
* Then run `docker-compose up`, which will start the container. 
* By default the dev server runs with uWSGI and NGINX with live-reloading enabled. If you'd like you can run the flask web server instead (see comments in the Dockerfile). If you choose to use NGINX you'll need to hop onto the container using `docker exec -it 'tabula' bash` and run NGINX in the background to expose uWSGI to your local machine. i.e. run `nginx &`. 
* If this is your first time running locally you'll need to run the database migrations and populate the database. All this requires is running (on the container) `python3 manage.py db upgrade && python3 populate_db.py`. See the comments in `populate_db.py` for more info on using Alembic. NB: If you kill the container inappropriately you may lose data in dev and need to run the migrations and populate the DB again. This isn't a huge deal; the whole process takes a couple seconds at most.
* When you're done run `docker-compose down` from the project root.

### Notes
* If you're not using the bare flask server, server logs will be found at `/var/logs/uwsgi.log` on the actual container. This is the same for prod. Feel free to change the log path in the Dockerfile. With flask they'll be flushed to the console.

## Prod
### Requirements

For the moment the deployment process is painfully manual. It's totally possible to automate, but not a priority for the moment.

* You'll need to install Github, Docker, docker-compose and potentially a couple other dependencies on the EC2 instance. Vim is also really helpful.
* Clone the repository on the box and copy over relevant files securely.
* All the same files need to be in the project root as for dev. Technically you can ignore the `.dev` file since only the `.env` is used in production. Likewise for `docker-compose.yml` vs. `docker-compose-prod.yml`. 
* Make sure to have the line `ssl on;` uncommented in the NGINX config. 
* The server expects there to be a running Postgres instance somewhere. This can be via RDS (as is currently the case) or it could be another container running on the same box (as is the case for dev), a postgres server running on EC2, etc. Ultimately comes down to cost vs. inconvenience. Paying for hours on RDS is expensive given that the project only gets hits at the beginning of the semester.
* Send Joe your public key so that he can grant you access to the server. You'll ssh on using something like `ssh -i ~/tabula_admin/ec2_private_key.pem ec2-user@54.89.178.72`.

### Getting started 

* Make sure you've got the latest code on the `master` branch. 
* Then you'll need to run `docker-compose -f docker-compose-prod.yml build` followed by `docker-compose -f docker-compose-prod.yml up -d`. The `-d` flag tells docker-compose to run everything in the background. 
* Hop onto the container and run NGINX in the background.
* If this is the first time you're setting up the server you'll need to run DB migrations and populate the DB in the same way you did for dev. In general you should at worst have to run the migrations, but should never be using `populate_db.py` in production since it doesn't check for existing data in an intelligent way. Much care is required for managing the DB. 

## Recommended reading 
* Authentication code was largely copied from [BitWiser](http://bitwiser.in/2015/09/09/add-google-login-in-flask.html)
* [Digital Ocean OAuth Guide](https://www.digitalocean.com/community/tutorials/an-introduction-to-oauth-2)
* [Alembic Tutorial](http://alembic.zzzcomputing.com/en/latest/tutorial.html)
* [Compose file reference](https://docs.docker.com/compose/compose-file/)
* [Dockerfile reference](https://docs.docker.com/engine/reference/builder/)
